{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOISFf2Cek+bnNfS3FQnipy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chetalispatil/Product_Analysis/blob/main/Product_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_oy5E7kF1O",
        "outputId": "37a5bff2-81ac-4157-a3c3-4a9174010e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk textblob scikit-learn imbalanced-learn matplotlib seaborn wordcloud\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch torchvision torchaudio\n",
        "!pip install -q datasets accelerate\n",
        "!pip install -q fastapi uvicorn pyngrok nest-asyncio\n",
        "!pip install -q beautifulsoup4 requests lxml html5lib fake-useragent\n",
        "!pip install -q pandas numpy matplotlib seaborn wordcloud\n",
        "!pip install -q scikit-learn nltk textblob spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxByMjel4obl",
        "outputId": "33e50553-0933-49c3-f6c4-2b2863d3696d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/161.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Standard libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "import requests\n",
        "from typing import List, Dict, Optional\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import time\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Transformers\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "# Web scraping\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# API\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, List\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "except:\n",
        "    print(\" SpaCy model not loaded. Run: !python -m spacy download en_core_web_sm\")\n",
        "    nlp = None\n",
        "\n",
        "print(\" All imports successful!\")"
      ],
      "metadata": {
        "id": "xySlU6HC4HmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProductReviewScraper:\n",
        "    \"\"\"Universal scraper for product reviews from e-commerce sites\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ua = UserAgent()\n",
        "        self.session = requests.Session()\n",
        "        self.headers = {\n",
        "            'User-Agent': self.ua.random,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1'\n",
        "        }\n",
        "\n",
        "    def identify_platform(self, url: str) -> str:\n",
        "        \"\"\"Identify e-commerce platform\"\"\"\n",
        "        url_lower = url.lower()\n",
        "        if 'amazon' in url_lower:\n",
        "            return 'amazon'\n",
        "        elif 'flipkart' in url_lower:\n",
        "            return 'flipkart'\n",
        "        elif 'ebay' in url_lower:\n",
        "            return 'ebay'\n",
        "        else:\n",
        "            return 'generic'\n",
        "\n",
        "    def scrape_amazon_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
        "        \"\"\"Scrape Amazon product reviews\"\"\"\n",
        "        try:\n",
        "            # Extract ASIN from URL\n",
        "            asin_match = re.search(r'/dp/([A-Z0-9]{10})', product_url)\n",
        "            if not asin_match:\n",
        "                asin_match = re.search(r'/product/([A-Z0-9]{10})', product_url)\n",
        "\n",
        "            if not asin_match:\n",
        "                print(\" Could not extract ASIN from URL\")\n",
        "                return self._get_demo_data()\n",
        "\n",
        "            asin = asin_match.group(1)\n",
        "            print(f\"âœ“ Found ASIN: {asin}\")\n",
        "\n",
        "            # Construct review URL\n",
        "            base_url = \"https://www.amazon.in\"\n",
        "            if \"amazon.com\" in product_url.lower():\n",
        "                base_url = \"https://www.amazon.com\"\n",
        "\n",
        "            review_url = f\"{base_url}/product-reviews/{asin}\"\n",
        "            reviews_data = []\n",
        "            page = 1\n",
        "\n",
        "            while len(reviews_data) < max_reviews and page <= 5:\n",
        "                try:\n",
        "                    print(f\"â†’ Scraping page {page}...\")\n",
        "\n",
        "                    params = {'pageNumber': page}\n",
        "                    response = self.session.get(\n",
        "                        review_url,\n",
        "                        headers=self.headers,\n",
        "                        params=params,\n",
        "                        timeout=15\n",
        "                    )\n",
        "\n",
        "                    if response.status_code != 200:\n",
        "                        print(f\"âš ï¸ Page {page} returned status {response.status_code}\")\n",
        "                        break\n",
        "\n",
        "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                    # Find review containers\n",
        "                    reviews = soup.find_all('div', {'data-hook': 'review'})\n",
        "\n",
        "                    if not reviews:\n",
        "                        print(f\" No reviews found on page {page}\")\n",
        "                        break\n",
        "\n",
        "                    for review in reviews:\n",
        "                        try:\n",
        "                            # Extract rating\n",
        "                            rating_elem = review.find('i', {'data-hook': 'review-star-rating'})\n",
        "                            if not rating_elem:\n",
        "                                rating_elem = review.find('i', class_='a-icon-star')\n",
        "\n",
        "                            rating = None\n",
        "                            if rating_elem:\n",
        "                                rating_text = rating_elem.get_text().strip()\n",
        "                                rating_match = re.search(r'(\\d+\\.?\\d*)', rating_text)\n",
        "                                if rating_match:\n",
        "                                    rating = float(rating_match.group(1))\n",
        "\n",
        "                            # Extract title\n",
        "                            title_elem = review.find('a', {'data-hook': 'review-title'})\n",
        "                            if not title_elem:\n",
        "                                title_elem = review.find('span', {'data-hook': 'review-title'})\n",
        "                            title = title_elem.get_text().strip() if title_elem else ''\n",
        "\n",
        "                            # Extract review text\n",
        "                            text_elem = review.find('span', {'data-hook': 'review-body'})\n",
        "                            text = text_elem.get_text().strip() if text_elem else ''\n",
        "\n",
        "                            # Extract date\n",
        "                            date_elem = review.find('span', {'data-hook': 'review-date'})\n",
        "                            date = date_elem.get_text().strip() if date_elem else ''\n",
        "\n",
        "                            # Extract verified purchase\n",
        "                            verified = review.find('span', {'data-hook': 'avp-badge'}) is not None\n",
        "\n",
        "                            # Only add if we have essential data\n",
        "                            if text and rating:\n",
        "                                reviews_data.append({\n",
        "                                    'rating': rating,\n",
        "                                    'title': title,\n",
        "                                    'review_text': text,\n",
        "                                    'date': date,\n",
        "                                    'verified': verified,\n",
        "                                    'platform': 'Amazon'\n",
        "                                })\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\" Error parsing single review: {str(e)[:50]}\")\n",
        "                            continue\n",
        "\n",
        "                    page += 1\n",
        "                    time.sleep(1)  # Be polite to servers\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\" Error on page {page}: {str(e)[:50]}\")\n",
        "                    break\n",
        "\n",
        "            if not reviews_data:\n",
        "                print(\" No reviews scraped. Using demo data.\")\n",
        "                return self._get_demo_data()\n",
        "\n",
        "            df = pd.DataFrame(reviews_data)\n",
        "            print(f\" Scraped {len(df)} reviews from Amazon\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error scraping Amazon: {str(e)[:100]}\")\n",
        "            print(\"â†’ Using demo data instead.\")\n",
        "            return self._get_demo_data()\n",
        "\n",
        "    def scrape_reviews(self, product_url: str, max_reviews: int = 50) -> pd.DataFrame:\n",
        "        \"\"\"Main method to scrape reviews\"\"\"\n",
        "        platform = self.identify_platform(product_url)\n",
        "        print(f\" Detected platform: {platform.upper()}\")\n",
        "\n",
        "        if platform == 'amazon':\n",
        "            return self.scrape_amazon_reviews(product_url, max_reviews)\n",
        "        else:\n",
        "            print(\" Platform not fully supported. Using demo data.\")\n",
        "            return self._get_demo_data()\n",
        "\n",
        "    def _get_demo_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Generate demo data for testing\"\"\"\n",
        "        demo_reviews = [\n",
        "            {\n",
        "                'rating': 5.0,\n",
        "                'title': 'Excellent Product!',\n",
        "                'review_text': 'This product exceeded my expectations. The quality is outstanding and delivery was super fast. Highly recommended for everyone!',\n",
        "                'verified': True,\n",
        "                'platform': 'Demo',\n",
        "                'date': 'January 15, 2025'\n",
        "            },\n",
        "            {\n",
        "                'rating': 4.0,\n",
        "                'title': 'Good value for money',\n",
        "                'review_text': 'Pretty good product overall. The build quality is solid and it works exactly as described. Minor issues with packaging but nothing major.',\n",
        "                'verified': True,\n",
        "                'platform': 'Demo',\n",
        "                'date': 'January 20, 2025'\n",
        "            },\n",
        "            {\n",
        "                'rating': 2.0,\n",
        "                'title': 'Disappointed with quality',\n",
        "                'review_text': 'The product quality is poor compared to the price. It stopped working after just a week. Customer service was unhelpful when I contacted them.',\n",
        "                'verified': True,\n",
        "                'platform': 'Demo',\n",
        "                'date': 'January 22, 2025'\n",
        "            },\n",
        "            {\n",
        "                'rating': 1.0,\n",
        "                'title': 'Complete waste of money',\n",
        "                'review_text': 'Terrible product. Completely broken on arrival and packaging was damaged. Do not buy this product. Save your hard-earned money.',\n",
        "                'verified': False,\n",
        "                'platform': 'Demo',\n",
        "                'date': 'January 25, 2025'\n",
        "            },\n",
        "            {\n",
        "                'rating': 3.0,\n",
        "                'title': 'Average product, nothing special',\n",
        "                'review_text': 'Its okay, nothing special about this product. Does the basic job but there are definitely better options available in the market.',\n",
        "                'verified': True,\n",
        "                'platform': 'Demo',\n",
        "                'date': 'January 28, 2025'\n",
        "            },\n",
        "            {\n",
        "                'rating': 5.0,\n",
        "                'title': 'Amazing quality!',\n",
        "                'review_text': 'Best purchase I made this year. The design is beautiful and performance is excellent. Fast delivery and great customer service too!',\n",
        "                'verified': True,\n",
        "                'platform': 'Demo',\n",
        "                'date': 'February 1, 2025'\n",
        "            },\n",
        "            {\n",
        "                'rating': 4.0,\n",
        "                'title': 'Satisfied customer',\n",
        "                'review_text': 'Good product with nice features. The price point is reasonable and quality is good. Would recommend to friends and family.',\n",
        "                'verified': True,\n",
        "                'platform': 'Demo',\n",
        "                'date': 'February 3, 2025'\n",
        "            },\n",
        "            {\n",
        "                'rating': 1.0,\n",
        "                'title': 'Very disappointed',\n",
        "                'review_text': 'Poor quality materials. Broke within days. Customer support did not respond to emails. Complete waste of time and money.',\n",
        "                'verified': True,\n",
        "                'platform': 'Demo',\n",
        "                'date': 'February 5, 2025'\n",
        "            }\n",
        "        ]\n",
        "   # Extend to reach desired number\n",
        "        extended_reviews = []\n",
        "        while len(extended_reviews) < 50:\n",
        "            extended_reviews.extend(demo_reviews)\n",
        "\n",
        "        return pd.DataFrame(extended_reviews[:50])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cNeDwKH14Nip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTSentimentAnalyzer:\n",
        "    \"\"\"Advanced sentiment analysis using BERT transformers\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='nlptown/bert-base-multilingual-uncased-sentiment'):\n",
        "        print(\" Loading BERT model...\")\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"â†’ Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            # Load pre-trained model\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            # Create pipeline\n",
        "            self.sentiment_pipeline = pipeline(\n",
        "                \"sentiment-analysis\",\n",
        "                model=self.model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                device=0 if torch.cuda.is_available() else -1,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            print(\" BERT model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading BERT model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict_sentiment(self, text: str) -> Dict:\n",
        "        \"\"\"Predict sentiment for a single text\"\"\"\n",
        "        try:\n",
        "            # Truncate text to avoid token limit issues\n",
        "            text = text[:512] if len(text) > 512 else text\n",
        "\n",
        "            result = self.sentiment_pipeline(text)[0]\n",
        "\n",
        "            # Extract star rating from label\n",
        "            label = result['label']\n",
        "            score = result['confidence'] if 'confidence' in result else result['score']\n",
        "\n",
        "            # Parse stars from label (format: \"X stars\")\n",
        "            stars_match = re.search(r'(\\d+)', label)\n",
        "            stars = int(stars_match.group(1)) if stars_match else 3\n",
        "\n",
        "            # Map to sentiment\n",
        "            if stars >= 4:\n",
        "                sentiment = 'Positive'\n",
        "            elif stars <= 2:\n",
        "                sentiment = 'Negative'\n",
        "            else:\n",
        "                sentiment = 'Neutral'\n",
        "\n",
        "            return {\n",
        "                'text': text[:100] + '...' if len(text) > 100 else text,\n",
        "                'sentiment': sentiment,\n",
        "                'stars': stars,\n",
        "                'confidence': float(score)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error in prediction: {e}\")\n",
        "            return {\n",
        "                'text': text[:100] if text else '',\n",
        "                'sentiment': 'Neutral',\n",
        "                'stars': 3,\n",
        "                'confidence': 0.5\n",
        "            }\n",
        "\n",
        "    def batch_predict(self, texts: List[str], batch_size: int = 8) -> List[Dict]:\n",
        "        \"\"\"Predict sentiment for multiple texts\"\"\"\n",
        "        results = []\n",
        "        total = len(texts)\n",
        "\n",
        "        print(f\" Analyzing {total} reviews...\")\n",
        "        for i in range(0, total, batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            for text in batch:\n",
        "                results.append(self.predict_sentiment(text))\n",
        "\n",
        "            if (i + batch_size) % 20 == 0:\n",
        "                print(f\"â†’ Processed {min(i + batch_size, total)}/{total} reviews\")\n",
        "\n",
        "        print(\" Batch prediction complete!\")\n",
        "        return results"
      ],
      "metadata": {
        "id": "in8JTMMg4SD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AspectBasedSentimentAnalyzer:\n",
        "    \"\"\"Extract aspects from reviews and analyze sentiment for each aspect\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = nlp\n",
        "\n",
        "        # Common product aspects\n",
        "        self.aspects = {\n",
        "            'quality': ['quality', 'build', 'material', 'durability', 'construction', 'made', 'sturdy'],\n",
        "            'price': ['price', 'cost', 'value', 'expensive', 'cheap', 'affordable', 'worth', 'money'],\n",
        "            'delivery': ['delivery', 'shipping', 'packaging', 'package', 'arrived', 'received', 'delivered'],\n",
        "            'performance': ['performance', 'works', 'working', 'speed', 'fast', 'slow', 'efficient', 'effective'],\n",
        "            'design': ['design', 'look', 'appearance', 'color', 'style', 'aesthetic', 'beautiful', 'ugly'],\n",
        "            'service': ['service', 'support', 'customer', 'help', 'response', 'helpful'],\n",
        "            'features': ['feature', 'functionality', 'option', 'capability', 'functions']\n",
        "        }\n",
        "\n",
        "    def extract_aspects(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract aspects mentioned in the text\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        found_aspects = []\n",
        "\n",
        "        for aspect, keywords in self.aspects.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in text_lower:\n",
        "                    found_aspects.append(aspect)\n",
        "                    break\n",
        "\n",
        "        return list(set(found_aspects))\n",
        "\n",
        "    def get_aspect_sentences(self, text: str, aspect: str) -> List[str]:\n",
        "        \"\"\"Get sentences that mention a specific aspect\"\"\"\n",
        "        if not self.nlp:\n",
        "            # Fallback if spaCy not loaded\n",
        "            sentences = text.split('.')\n",
        "        else:\n",
        "            doc = self.nlp(text)\n",
        "            sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "        aspect_keywords = self.aspects.get(aspect, [])\n",
        "\n",
        "        relevant_sentences = []\n",
        "        for sentence in sentences:\n",
        "            sentence_lower = sentence.lower()\n",
        "            if any(keyword in sentence_lower for keyword in aspect_keywords):\n",
        "                relevant_sentences.append(sentence)\n",
        "\n",
        "        return relevant_sentences\n",
        "\n",
        "    def analyze_aspect_sentiment(self, text: str, bert_analyzer) -> Dict:\n",
        "        \"\"\"Analyze sentiment for each aspect in the text\"\"\"\n",
        "        aspects = self.extract_aspects(text)\n",
        "        aspect_sentiments = {}\n",
        "\n",
        "        for aspect in aspects:\n",
        "            sentences = self.get_aspect_sentences(text, aspect)\n",
        "            if sentences:\n",
        "                # Analyze sentiment of aspect-related sentences\n",
        "                combined_text = ' '.join(sentences)\n",
        "                sentiment_result = bert_analyzer.predict_sentiment(combined_text)\n",
        "                aspect_sentiments[aspect] = {\n",
        "                    'sentiment': sentiment_result['sentiment'],\n",
        "                    'confidence': sentiment_result['confidence'],\n",
        "                    'mentions': sentences[:2]  # First 2 mentions\n",
        "                }\n",
        "\n",
        "        return aspect_sentiments\n",
        "\n",
        "    def analyze_reviews_aspects(self, df: pd.DataFrame, bert_analyzer) -> pd.DataFrame:\n",
        "        \"\"\"Analyze aspects for all reviews\"\"\"\n",
        "        print(\"ðŸ”„ Analyzing aspects for all reviews...\")\n",
        "\n",
        "        aspect_data = []\n",
        "        total = len(df)\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            text = row['review_text']\n",
        "            aspects = self.analyze_aspect_sentiment(text, bert_analyzer)\n",
        "\n",
        "            aspect_data.append({\n",
        "                'review_index': idx,\n",
        "                'aspects': aspects,\n",
        "                'num_aspects': len(aspects)\n",
        "            })\n",
        "\n",
        "            if (idx + 1) % 10 == 0:\n",
        "                print(f\"â†’ Analyzed {idx + 1}/{total} reviews\")\n",
        "\n",
        "        print(\"âœ… Aspect analysis complete!\")\n",
        "        return pd.DataFrame(aspect_data)"
      ],
      "metadata": {
        "id": "qNa1AiLw4T2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_visualizations(df_reviews, df_aspects):\n",
        "    \"\"\"Create comprehensive visualizations\"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # 1. Sentiment Distribution Pie Chart\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    sentiment_counts = df_reviews['bert_sentiment'].value_counts()\n",
        "    colors = {'Positive': '#2ecc71', 'Negative': '#e74c3c', 'Neutral': '#95a5a6'}\n",
        "    sentiment_colors = [colors.get(s, '#3498db') for s in sentiment_counts.index]\n",
        "    ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
        "            colors=sentiment_colors, startangle=90)\n",
        "    ax1.set_title('Sentiment Distribution', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # 2. Rating vs Sentiment\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    rating_sentiment = pd.crosstab(df_reviews['rating'], df_reviews['bert_sentiment'])\n",
        "    rating_sentiment.plot(kind='bar', ax=ax2, color=['#e74c3c', '#95a5a6', '#2ecc71'])\n",
        "    ax2.set_title('Rating vs BERT Sentiment', fontsize=12, fontweight='bold')\n",
        "    ax2.set_xlabel('Rating')\n",
        "    ax2.set_ylabel('Count')\n",
        "    ax2.legend(title='Sentiment', loc='upper left')\n",
        "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=0)\n",
        "\n",
        "    # 3. Confidence Distribution\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    ax3.hist(df_reviews['bert_confidence'], bins=20, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    ax3.set_title('BERT Confidence Distribution', fontsize=12, fontweight='bold')\n",
        "    ax3.set_xlabel('Confidence Score')\n",
        "    ax3.set_ylabel('Frequency')\n",
        "    mean_conf = df_reviews['bert_confidence'].mean()\n",
        "    ax3.axvline(mean_conf, color='red', linestyle='--',\n",
        "                label=f'Mean: {mean_conf:.3f}')\n",
        "    ax3.legend()\n",
        "\n",
        "    # 4. Aspect Frequency\n",
        "    ax4 = fig.add_subplot(gs[1, :])\n",
        "    all_aspects = []\n",
        "    for aspects_dict in df_aspects['aspects']:\n",
        "        if isinstance(aspects_dict, dict):\n",
        "            all_aspects.extend(aspects_dict.keys())\n",
        "\n",
        "    if all_aspects:\n",
        "        aspect_counts = pd.Series(all_aspects).value_counts()\n",
        "        aspect_counts.plot(kind='barh', ax=ax4, color='#9b59b6')\n",
        "        ax4.set_title('Most Discussed Aspects', fontsize=12, fontweight='bold')\n",
        "        ax4.set_xlabel('Frequency')\n",
        "        ax4.set_ylabel('Aspect')\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'No aspects found', ha='center', va='center')\n",
        "        ax4.set_title('Most Discussed Aspects', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # 5. Positive Reviews Word Cloud\n",
        "    ax5 = fig.add_subplot(gs[2, 0])\n",
        "    positive_reviews = df_reviews[df_reviews['bert_sentiment'] == 'Positive']\n",
        "    if len(positive_reviews) > 0:\n",
        "        positive_text = ' '.join(positive_reviews['review_text'].astype(str))\n",
        "        wordcloud = WordCloud(width=400, height=300, background_color='white',\n",
        "                             colormap='Greens', max_words=50).generate(positive_text)\n",
        "        ax5.imshow(wordcloud, interpolation='bilinear')\n",
        "    else:\n",
        "        ax5.text(0.5, 0.5, 'No positive reviews', ha='center', va='center')\n",
        "    ax5.set_title('Positive Reviews Word Cloud', fontsize=12, fontweight='bold')\n",
        "    ax5.axis('off')\n",
        "\n",
        "    # 6. Negative Reviews Word Cloud\n",
        "    ax6 = fig.add_subplot(gs[2, 1])\n",
        "    negative_reviews = df_reviews[df_reviews['bert_sentiment'] == 'Negative']\n",
        "    if len(negative_reviews) > 0:\n",
        "        negative_text = ' '.join(negative_reviews['review_text'].astype(str))\n",
        "        wordcloud = WordCloud(width=400, height=300, background_color='white',\n",
        "                             colormap='Reds', max_words=50).generate(negative_text)\n",
        "        ax6.imshow(wordcloud, interpolation='bilinear')\n",
        "    else:\n",
        "        ax6.text(0.5, 0.5, 'No negative reviews', ha='center', va='center')\n",
        "    ax6.set_title('Negative Reviews Word Cloud', fontsize=12, fontweight='bold')\n",
        "    ax6.axis('off')\n",
        "\n",
        "    # 7. Sentiment by Platform\n",
        "    ax7 = fig.add_subplot(gs[2, 2])\n",
        "    if 'platform' in df_reviews.columns:\n",
        "        platform_sentiment = pd.crosstab(df_reviews['platform'], df_reviews['bert_sentiment'])\n",
        "        platform_sentiment.plot(kind='bar', ax=ax7, color=['#e74c3c', '#95a5a6', '#2ecc71'])\n",
        "        ax7.set_title('Sentiment by Platform', fontsize=12, fontweight='bold')\n",
        "        ax7.set_xlabel('Platform')\n",
        "        ax7.set_ylabel('Count')\n",
        "        ax7.legend(title='Sentiment', loc='upper left')\n",
        "        plt.setp(ax7.xaxis.get_majorticklabels(), rotation=45)\n",
        "\n",
        "    plt.suptitle(' Product Review Analysis Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\" Visualizations created!\")"
      ],
      "metadata": {
        "id": "A4oxuKrB4axO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewAnalytics:\n",
        "    \"\"\"Generate statistical insights from reviews\"\"\"\n",
        "\n",
        "    def __init__(self, df_reviews, df_aspects):\n",
        "        self.df_reviews = df_reviews\n",
        "        self.df_aspects = df_aspects\n",
        "\n",
        "    def get_overall_statistics(self) -> Dict:\n",
        "        \"\"\"Calculate overall statistics\"\"\"\n",
        "        stats = {\n",
        "            'total_reviews': len(self.df_reviews),\n",
        "            'average_rating': self.df_reviews['rating'].mean(),\n",
        "            'median_rating': self.df_reviews['rating'].median(),\n",
        "            'sentiment_distribution': self.df_reviews['bert_sentiment'].value_counts().to_dict(),\n",
        "            'positive_percentage': (self.df_reviews['bert_sentiment'] == 'Positive').mean() * 100,\n",
        "            'negative_percentage': (self.df_reviews['bert_sentiment'] == 'Negative').mean() * 100,\n",
        "            'neutral_percentage': (self.df_reviews['bert_sentiment'] == 'Neutral').mean() * 100,\n",
        "            'average_confidence': self.df_reviews['bert_confidence'].mean(),\n",
        "            'verified_reviews': self.df_reviews['verified'].sum() if 'verified' in self.df_reviews.columns else 0\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def get_aspect_statistics(self) -> Dict:\n",
        "        \"\"\"Calculate aspect-level statistics\"\"\"\n",
        "        all_aspects = []\n",
        "        aspect_sentiments = {'Positive': [], 'Negative': [], 'Neutral': []}\n",
        "\n",
        "        for aspects_dict in self.df_aspects['aspects']:\n",
        "            if isinstance(aspects_dict, dict):\n",
        "                for aspect, details in aspects_dict.items():\n",
        "                    all_aspects.append(aspect)\n",
        "                    if isinstance(details, dict) and 'sentiment' in details:\n",
        "                        sentiment = details['sentiment']\n",
        "                        aspect_sentiments[sentiment].append(aspect)\n",
        "\n",
        "        aspect_counts = Counter(all_aspects)\n",
        "\n",
        "        stats = {\n",
        "            'most_discussed_aspects': dict(aspect_counts.most_common(5)),\n",
        "            'total_aspect_mentions': len(all_aspects),\n",
        "            'positive_aspects': dict(Counter(aspect_sentiments['Positive']).most_common(3)),\n",
        "            'negative_aspects': dict(Counter(aspect_sentiments['Negative']).most_common(3)),\n",
        "            'average_aspects_per_review': self.df_aspects['num_aspects'].mean()\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def get_rating_breakdown(self) -> pd.DataFrame:\n",
        "        \"\"\"Get detailed rating breakdown\"\"\"\n",
        "        rating_breakdown = self.df_reviews.groupby('rating').agg({\n",
        "            'review_text': 'count',\n",
        "            'bert_confidence': 'mean',\n",
        "            'bert_sentiment': lambda x: x.value_counts().to_dict()\n",
        "        }).reset_index()\n",
        "        rating_breakdown.columns = ['Rating', 'Count', 'Avg_Confidence', 'Sentiment_Distribution']\n",
        "        return rating_breakdown\n",
        "\n",
        "    def identify_key_strengths(self, top_n: int = 5) -> List[str]:\n",
        "        \"\"\"Identify key product strengths from positive reviews\"\"\"\n",
        "        positive_reviews = self.df_reviews[self.df_reviews['bert_sentiment'] == 'Positive']\n",
        "\n",
        "        if len(positive_reviews) == 0:\n",
        "            return [\"No positive reviews found\"]\n",
        "\n",
        "        # Get positive aspect mentions\n",
        "        positive_aspects = []\n",
        "        for idx in positive_reviews.index:\n",
        "            if idx < len(self.df_aspects):\n",
        "                aspects_dict = self.df_aspects.iloc[idx]['aspects']\n",
        "                if isinstance(aspects_dict, dict):\n",
        "                    for aspect, details in aspects_dict.items():\n",
        "                        if isinstance(details, dict) and details.get('sentiment') == 'Positive':\n",
        "                            positive_aspects.append(aspect)\n",
        "\n",
        "        if not positive_aspects:\n",
        "            return [\"High customer satisfaction\", \"Positive overall feedback\"]\n",
        "\n",
        "        strength_counts = Counter(positive_aspects)\n",
        "        strengths = [f\"{aspect.title()} ({count} mentions)\"\n",
        "                    for aspect, count in strength_counts.most_common(top_n)]\n",
        "        return strengths\n",
        "\n",
        "    def identify_key_weaknesses(self, top_n: int = 5) -> List[str]:\n",
        "        \"\"\"Identify key product weaknesses from negative reviews\"\"\"\n",
        "        negative_reviews = self.df_reviews[self.df_reviews['bert_sentiment'] == 'Negative']\n",
        "\n",
        "        if len(negative_reviews) == 0:\n",
        "            return [\"No significant weaknesses identified\"]\n",
        "\n",
        "        # Get negative aspect mentions\n",
        "        negative_aspects = []\n",
        "        for idx in negative_reviews.index:\n",
        "            if idx < len(self.df_aspects):\n",
        "                aspects_dict = self.df_aspects.iloc[idx]['aspects']\n",
        "                if isinstance(aspects_dict, dict):\n",
        "                    for aspect, details in aspects_dict.items():\n",
        "                        if isinstance(details, dict) and details.get('sentiment') == 'Negative':\n",
        "                            negative_aspects.append(aspect)\n",
        "\n",
        "        if not negative_aspects:\n",
        "            return [\"Minor concerns raised by some customers\"]\n",
        "\n",
        "        weakness_counts = Counter(negative_aspects)\n",
        "        weaknesses = [f\"{aspect.title()} ({count} mentions)\"\n",
        "                     for aspect, count in weakness_counts.most_common(top_n)]\n",
        "        return weaknesses\n",
        "\n",
        "    def generate_summary_report(self) -> str:\n",
        "        \"\"\"Generate a comprehensive summary report\"\"\"\n",
        "        stats = self.get_overall_statistics()\n",
        "        aspect_stats = self.get_aspect_statistics()\n",
        "        strengths = self.identify_key_strengths()\n",
        "        weaknesses = self.identify_key_weaknesses()\n",
        "\n",
        "        report = f\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘             PRODUCT REVIEW ANALYSIS SUMMARY REPORT               â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        " OVERALL STATISTICS\n",
        "{'â”€' * 66}\n",
        "â€¢ Total Reviews Analyzed: {stats['total_reviews']}\n",
        "â€¢ Average Rating: {stats['average_rating']:.2f} / 5.0\n",
        "â€¢ Median Rating: {stats['median_rating']:.1f}\n",
        "â€¢ Average Confidence Score: {stats['average_confidence']:.3f}\n",
        "\n",
        " SENTIMENT BREAKDOWN\n",
        "{'â”€' * 66}\n",
        "â€¢ Positive Reviews: {stats['positive_percentage']:.1f}%\n",
        "â€¢ Negative Reviews: {stats['negative_percentage']:.1f}%\n",
        "â€¢ Neutral Reviews: {stats['neutral_percentage']:.1f}%\n",
        "\n",
        " MOST DISCUSSED ASPECTS\n",
        "{'â”€' * 66}\n",
        "\"\"\"\n",
        "        for aspect, count in list(aspect_stats['most_discussed_aspects'].items())[:5]:\n",
        "            report += f\"â€¢ {aspect.title()}: {count} mentions\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        "âœ… KEY STRENGTHS\n",
        "{'â”€' * 66}\n",
        "\"\"\"\n",
        "        for i, strength in enumerate(strengths, 1):\n",
        "            report += f\"{i}. {strength}\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        " KEY WEAKNESSES\n",
        "{'â”€' * 66}\n",
        "\"\"\"\n",
        "        for i, weakness in enumerate(weaknesses, 1):\n",
        "            report += f\"{i}. {weakness}\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        " RECOMMENDATIONS\n",
        "{'â”€' * 66}\n",
        "\"\"\"\n",
        "        if stats['positive_percentage'] >= 60:\n",
        "            report += \"â€¢ Product has strong positive reception from customers\\n\"\n",
        "        elif stats['negative_percentage'] >= 40:\n",
        "            report += \"â€¢ Product needs significant improvements to satisfy customers\\n\"\n",
        "        else:\n",
        "            report += \"â€¢ Product has mixed reviews, focus on addressing concerns\\n\"\n",
        "\n",
        "        if stats['average_rating'] >= 4.0:\n",
        "            report += \"â€¢ Maintain current quality standards\\n\"\n",
        "        elif stats['average_rating'] < 3.0:\n",
        "            report += \"â€¢ Urgent: Address quality and customer satisfaction issues\\n\"\n",
        "\n",
        "        report += \"\\n\" + \"â•\" * 66 + \"\\n\"\n",
        "        return report\n",
        "\n",
        "    def print_statistics(self):\n",
        "        \"\"\"Print all statistics and insights\"\"\"\n",
        "        print(self.generate_summary_report())\n"
      ],
      "metadata": {
        "id": "sZU85rfH4iRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewExporter:\n",
        "    \"\"\"Export analysis results to various formats\"\"\"\n",
        "\n",
        "    def __init__(self, df_reviews, df_aspects, analytics):\n",
        "        self.df_reviews = df_reviews\n",
        "        self.df_aspects = df_aspects\n",
        "        self.analytics = analytics\n",
        "\n",
        "    def export_to_csv(self, filename: str = 'review_analysis.csv'):\n",
        "        \"\"\"Export main results to CSV\"\"\"\n",
        "        try:\n",
        "            # Merge reviews with aspects\n",
        "            export_df = self.df_reviews.copy()\n",
        "\n",
        "            # Add aspect information\n",
        "            export_df['num_aspects'] = self.df_aspects['num_aspects']\n",
        "            export_df['aspects_discussed'] = self.df_aspects['aspects'].apply(\n",
        "                lambda x: ', '.join(x.keys()) if isinstance(x, dict) else ''\n",
        "            )\n",
        "\n",
        "            export_df.to_csv(filename, index=False)\n",
        "            print(f\"âœ… Exported to {filename}\")\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error exporting to CSV: {e}\")\n",
        "            return None\n",
        "\n",
        "    def export_to_json(self, filename: str = 'review_analysis.json'):\n",
        "        \"\"\"Export detailed results to JSON\"\"\"\n",
        "        try:\n",
        "            export_data = {\n",
        "                'metadata': {\n",
        "                    'export_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'total_reviews': len(self.df_reviews)\n",
        "                },\n",
        "                'statistics': self.analytics.get_overall_statistics(),\n",
        "                'aspect_statistics': self.analytics.get_aspect_statistics(),\n",
        "                'reviews': []\n",
        "            }\n",
        "\n",
        "            for idx, row in self.df_reviews.iterrows():\n",
        "                review_data = {\n",
        "                    'rating': float(row['rating']),\n",
        "                    'title': str(row['title']),\n",
        "                    'review_text': str(row['review_text']),\n",
        "                    'bert_sentiment': str(row['bert_sentiment']),\n",
        "                    'bert_confidence': float(row['bert_confidence']),\n",
        "                    'verified': bool(row['verified']) if 'verified' in row else False\n",
        "                }\n",
        "\n",
        "                if idx < len(self.df_aspects):\n",
        "                    review_data['aspects'] = self.df_aspects.iloc[idx]['aspects']\n",
        "\n",
        "                export_data['reviews'].append(review_data)\n",
        "\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            print(f\"âœ… Exported to {filename}\")\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error exporting to JSON: {e}\")\n",
        "            return None\n",
        "\n",
        "    def export_summary_report(self, filename: str = 'summary_report.txt'):\n",
        "        \"\"\"Export summary report to text file\"\"\"\n",
        "        try:\n",
        "            report = self.analytics.generate_summary_report()\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(report)\n",
        "            print(f\" Exported summary report to {filename}\")\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            print(f\" Error exporting summary: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "aHBWH2ca4yDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Models\n",
        "class ReviewInput(BaseModel):\n",
        "    text: str\n",
        "    rating: Optional[float] = None\n",
        "\n",
        "class ProductURL(BaseModel):\n",
        "    url: str\n",
        "    max_reviews: int = 50\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI(title=\"Product Review Sentiment Analysis API\")\n",
        "\n",
        "# Global variables to store models\n",
        "bert_analyzer = None\n",
        "aspect_analyzer = None\n",
        "scraper = None\n",
        "\n",
        "def initialize_models():\n",
        "    \"\"\"Initialize all models\"\"\"\n",
        "    global bert_analyzer, aspect_analyzer, scraper\n",
        "\n",
        "    if bert_analyzer is None:\n",
        "        bert_analyzer = BERTSentimentAnalyzer()\n",
        "    if aspect_analyzer is None:\n",
        "        aspect_analyzer = AspectBasedSentimentAnalyzer()\n",
        "    if scraper is None:\n",
        "        scraper = ProductReviewScraper()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\n",
        "        \"message\": \"Product Review Sentiment Analysis API\",\n",
        "        \"endpoints\": {\n",
        "            \"/analyze\": \"POST - Analyze a single review\",\n",
        "            \"/analyze-product\": \"POST - Scrape and analyze product reviews\",\n",
        "            \"/health\": \"GET - Check API health\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"healthy\", \"models_loaded\": bert_analyzer is not None}\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "def analyze_single_review(review: ReviewInput):\n",
        "    \"\"\"Analyze a single review\"\"\"\n",
        "    try:\n",
        "        initialize_models()\n",
        "\n",
        "        # Get sentiment\n",
        "        sentiment_result = bert_analyzer.predict_sentiment(review.text)\n",
        "\n",
        "        # Get aspects\n",
        "        aspects = aspect_analyzer.analyze_aspect_sentiment(review.text, bert_analyzer)\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"sentiment\": sentiment_result['sentiment'],\n",
        "            \"confidence\": sentiment_result['confidence'],\n",
        "            \"stars_predicted\": sentiment_result['stars'],\n",
        "            \"aspects\": aspects,\n",
        "            \"rating_provided\": review.rating\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/analyze-product\")\n",
        "def analyze_product_reviews(product: ProductURL):\n",
        "    \"\"\"Scrape and analyze product reviews\"\"\"\n",
        "    try:\n",
        "        initialize_models()\n",
        "\n",
        "        # Scrape reviews\n",
        "        df_reviews = scraper.scrape_reviews(product.url, product.max_reviews)\n",
        "\n",
        "        # Analyze sentiments\n",
        "        texts = df_reviews['review_text'].tolist()\n",
        "        sentiment_results = bert_analyzer.batch_predict(texts)\n",
        "\n",
        "        df_reviews['bert_sentiment'] = [r['sentiment'] for r in sentiment_results]\n",
        "        df_reviews['bert_confidence'] = [r['confidence'] for r in sentiment_results]\n",
        "        df_reviews['bert_stars'] = [r['stars'] for r in sentiment_results]\n",
        "\n",
        "        # Analyze aspects\n",
        "        df_aspects = aspect_analyzer.analyze_reviews_aspects(df_reviews, bert_analyzer)\n",
        "\n",
        "        # Get statistics\n",
        "        analytics = ReviewAnalytics(df_reviews, df_aspects)\n",
        "        stats = analytics.get_overall_statistics()\n",
        "        aspect_stats = analytics.get_aspect_statistics()\n",
        "\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"total_reviews\": len(df_reviews),\n",
        "            \"statistics\": stats,\n",
        "            \"aspect_statistics\": aspect_stats,\n",
        "            \"top_positive_reviews\": df_reviews[df_reviews['bert_sentiment'] == 'Positive'].head(3)[['title', 'review_text']].to_dict('records'),\n",
        "            \"top_negative_reviews\": df_reviews[df_reviews['bert_sentiment'] == 'Negative'].head(3)[['title', 'review_text']].to_dict('records')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ],
      "metadata": {
        "id": "sT7ccCrS4yfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_complete_analysis(product_url: str, max_reviews: int = 50):\n",
        "\n",
        "    print(\"STARTING PRODUCT REVIEW SENTIMENT ANALYSIS\")\n",
        "\n",
        "    # Step 1: Scrape reviews\n",
        "    print(\"\\n STEP 1: Scraping Reviews\")\n",
        "    print(\"-\" * 70)\n",
        "    scraper = ProductReviewScraper()\n",
        "    df_reviews = scraper.scrape_reviews(product_url, max_reviews)\n",
        "    print(f\" Scraped {len(df_reviews)} reviews\\n\")\n",
        "\n",
        "    # Step 2: Initialize BERT model\n",
        "    print(\" STEP 2: Initializing BERT Model\")\n",
        "    print(\"-\" * 70)\n",
        "    bert_analyzer = BERTSentimentAnalyzer()\n",
        "    print()\n",
        "\n",
        "    # Step 3: Analyze sentiments\n",
        "    print(\" STEP 3: Analyzing Sentiments\")\n",
        "    print(\"-\" * 70)\n",
        "    texts = df_reviews['review_text'].tolist()\n",
        "    sentiment_results = bert_analyzer.batch_predict(texts)\n",
        "\n",
        "    df_reviews['bert_sentiment'] = [r['sentiment'] for r in sentiment_results]\n",
        "    df_reviews['bert_confidence'] = [r['confidence'] for r in sentiment_results]\n",
        "    df_reviews['bert_stars'] = [r['stars'] for r in sentiment_results]\n",
        "    print()\n",
        "\n",
        "    # Step 4: Aspect-based analysis\n",
        "    print(\" STEP 4: Aspect-Based Analysis\")\n",
        "    print(\"-\" * 70)\n",
        "    aspect_analyzer = AspectBasedSentimentAnalyzer()\n",
        "    df_aspects = aspect_analyzer.analyze_reviews_aspects(df_reviews, bert_analyzer)\n",
        "    print()\n",
        "\n",
        "    # Step 5: Generate analytics\n",
        "    print(\"ðŸ“Š STEP 5: Generating Analytics\")\n",
        "    print(\"-\" * 70)\n",
        "    analytics = ReviewAnalytics(df_reviews, df_aspects)\n",
        "    analytics.print_statistics()\n",
        "\n",
        "    # Step 6: Create visualizations\n",
        "    print(\" STEP 6: Creating Visualizations\")\n",
        "    print(\"-\" * 70)\n",
        "    create_visualizations(df_reviews, df_aspects)\n",
        "\n",
        "    # Step 7: Export results\n",
        "    print(\"\\n STEP 7: Exporting Results\")\n",
        "    print(\"-\" * 70)\n",
        "    exporter = ReviewExporter(df_reviews, df_aspects, analytics)\n",
        "    exporter.export_to_csv()\n",
        "    exporter.export_to_json()\n",
        "    exporter.export_summary_report()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ANALYSIS COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return df_reviews, df_aspects, analytics\n",
        "\n",
        "\n",
        "print(\"\\n All code loaded successfully!\")\n",
        "print(\" Use run_complete_analysis() to start the analysis pipeline\")"
      ],
      "metadata": {
        "id": "Je6i_ioT41WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Start - Complete Analysis\n",
        "product_url = \"https://amzn.in/d/4oQztnZ\"\n",
        "df_reviews, df_aspects, analytics = run_complete_analysis(product_url, max_reviews=30)\n",
        "\n",
        "# Display results\n",
        "print(df_reviews.head())\n",
        "print(analytics.generate_summary_report())"
      ],
      "metadata": {
        "id": "9h4_xfG27CzC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}